{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VEQ1LoDkzTW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import NullFormatter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8FmcZ_-k0ks"
      },
      "source": [
        "df=pd.read_csv(\"filtered_signals.csv\")\n",
        "df = df.drop('Unnamed: 0', axis = 1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "pFdiGiegliil",
        "outputId": "2c0db03c-41c9-4a9d-9d26-32ecdfcf9da0"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>4058</th>\n",
              "      <th>4059</th>\n",
              "      <th>4060</th>\n",
              "      <th>4061</th>\n",
              "      <th>4062</th>\n",
              "      <th>4063</th>\n",
              "      <th>4064</th>\n",
              "      <th>4065</th>\n",
              "      <th>4066</th>\n",
              "      <th>4067</th>\n",
              "      <th>4068</th>\n",
              "      <th>4069</th>\n",
              "      <th>4070</th>\n",
              "      <th>4071</th>\n",
              "      <th>4072</th>\n",
              "      <th>4073</th>\n",
              "      <th>4074</th>\n",
              "      <th>4075</th>\n",
              "      <th>4076</th>\n",
              "      <th>4077</th>\n",
              "      <th>4078</th>\n",
              "      <th>4079</th>\n",
              "      <th>4080</th>\n",
              "      <th>4081</th>\n",
              "      <th>4082</th>\n",
              "      <th>4083</th>\n",
              "      <th>4084</th>\n",
              "      <th>4085</th>\n",
              "      <th>4086</th>\n",
              "      <th>4087</th>\n",
              "      <th>4088</th>\n",
              "      <th>4089</th>\n",
              "      <th>4090</th>\n",
              "      <th>4091</th>\n",
              "      <th>4092</th>\n",
              "      <th>4093</th>\n",
              "      <th>4094</th>\n",
              "      <th>4095</th>\n",
              "      <th>4096</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-15.979399</td>\n",
              "      <td>-25.154399</td>\n",
              "      <td>-39.171428</td>\n",
              "      <td>-49.904093</td>\n",
              "      <td>-49.500085</td>\n",
              "      <td>-43.865513</td>\n",
              "      <td>-40.342316</td>\n",
              "      <td>-40.571842</td>\n",
              "      <td>-48.234938</td>\n",
              "      <td>-54.454867</td>\n",
              "      <td>-44.893013</td>\n",
              "      <td>-24.711255</td>\n",
              "      <td>-3.378817</td>\n",
              "      <td>20.898695</td>\n",
              "      <td>45.259817</td>\n",
              "      <td>55.065315</td>\n",
              "      <td>45.036407</td>\n",
              "      <td>20.259775</td>\n",
              "      <td>-10.393070</td>\n",
              "      <td>-38.170744</td>\n",
              "      <td>-59.818828</td>\n",
              "      <td>-68.728323</td>\n",
              "      <td>-61.590045</td>\n",
              "      <td>-41.671593</td>\n",
              "      <td>-18.197099</td>\n",
              "      <td>-6.815934</td>\n",
              "      <td>-5.136747</td>\n",
              "      <td>5.200362</td>\n",
              "      <td>32.721774</td>\n",
              "      <td>64.581253</td>\n",
              "      <td>78.204246</td>\n",
              "      <td>62.159562</td>\n",
              "      <td>23.220325</td>\n",
              "      <td>-18.269025</td>\n",
              "      <td>-45.201232</td>\n",
              "      <td>-53.108771</td>\n",
              "      <td>-51.658074</td>\n",
              "      <td>-48.233315</td>\n",
              "      <td>-44.490971</td>\n",
              "      <td>-44.288040</td>\n",
              "      <td>...</td>\n",
              "      <td>20.251387</td>\n",
              "      <td>23.415356</td>\n",
              "      <td>41.614580</td>\n",
              "      <td>68.788502</td>\n",
              "      <td>89.607979</td>\n",
              "      <td>95.186039</td>\n",
              "      <td>86.807099</td>\n",
              "      <td>69.632194</td>\n",
              "      <td>50.220419</td>\n",
              "      <td>35.598547</td>\n",
              "      <td>25.438487</td>\n",
              "      <td>17.578748</td>\n",
              "      <td>16.138595</td>\n",
              "      <td>22.419366</td>\n",
              "      <td>32.033196</td>\n",
              "      <td>37.478490</td>\n",
              "      <td>33.269596</td>\n",
              "      <td>20.747507</td>\n",
              "      <td>2.532114</td>\n",
              "      <td>-16.599436</td>\n",
              "      <td>-27.385645</td>\n",
              "      <td>-24.697252</td>\n",
              "      <td>-10.401483</td>\n",
              "      <td>8.816674</td>\n",
              "      <td>27.193302</td>\n",
              "      <td>47.534502</td>\n",
              "      <td>74.735355</td>\n",
              "      <td>103.498752</td>\n",
              "      <td>119.037487</td>\n",
              "      <td>113.166957</td>\n",
              "      <td>96.810241</td>\n",
              "      <td>89.754186</td>\n",
              "      <td>96.795158</td>\n",
              "      <td>107.075249</td>\n",
              "      <td>111.498484</td>\n",
              "      <td>103.900505</td>\n",
              "      <td>90.135530</td>\n",
              "      <td>75.137893</td>\n",
              "      <td>51.276260</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-9.449462</td>\n",
              "      <td>-15.923934</td>\n",
              "      <td>-23.698261</td>\n",
              "      <td>-31.104874</td>\n",
              "      <td>-34.974588</td>\n",
              "      <td>-35.664772</td>\n",
              "      <td>-35.131378</td>\n",
              "      <td>-35.841482</td>\n",
              "      <td>-36.513342</td>\n",
              "      <td>-32.052458</td>\n",
              "      <td>-23.257437</td>\n",
              "      <td>-14.061514</td>\n",
              "      <td>-6.484579</td>\n",
              "      <td>-0.384956</td>\n",
              "      <td>1.908656</td>\n",
              "      <td>-1.624724</td>\n",
              "      <td>-6.069053</td>\n",
              "      <td>-5.123772</td>\n",
              "      <td>1.506732</td>\n",
              "      <td>11.793131</td>\n",
              "      <td>24.705315</td>\n",
              "      <td>33.695722</td>\n",
              "      <td>34.156805</td>\n",
              "      <td>29.612231</td>\n",
              "      <td>24.707594</td>\n",
              "      <td>22.656870</td>\n",
              "      <td>27.126809</td>\n",
              "      <td>37.021607</td>\n",
              "      <td>45.537554</td>\n",
              "      <td>47.623723</td>\n",
              "      <td>43.832166</td>\n",
              "      <td>37.641254</td>\n",
              "      <td>27.900083</td>\n",
              "      <td>16.789820</td>\n",
              "      <td>11.587675</td>\n",
              "      <td>13.564736</td>\n",
              "      <td>21.118909</td>\n",
              "      <td>32.078287</td>\n",
              "      <td>42.061934</td>\n",
              "      <td>47.121655</td>\n",
              "      <td>...</td>\n",
              "      <td>-36.308060</td>\n",
              "      <td>-38.813778</td>\n",
              "      <td>-46.766355</td>\n",
              "      <td>-56.661182</td>\n",
              "      <td>-63.538010</td>\n",
              "      <td>-65.955288</td>\n",
              "      <td>-67.090762</td>\n",
              "      <td>-69.777873</td>\n",
              "      <td>-73.621886</td>\n",
              "      <td>-79.616675</td>\n",
              "      <td>-86.601571</td>\n",
              "      <td>-89.501122</td>\n",
              "      <td>-86.058744</td>\n",
              "      <td>-80.527219</td>\n",
              "      <td>-76.215091</td>\n",
              "      <td>-75.369721</td>\n",
              "      <td>-81.742958</td>\n",
              "      <td>-93.248630</td>\n",
              "      <td>-101.396848</td>\n",
              "      <td>-100.011726</td>\n",
              "      <td>-90.976177</td>\n",
              "      <td>-79.060597</td>\n",
              "      <td>-70.943880</td>\n",
              "      <td>-72.570980</td>\n",
              "      <td>-80.530017</td>\n",
              "      <td>-87.263208</td>\n",
              "      <td>-91.868476</td>\n",
              "      <td>-98.376186</td>\n",
              "      <td>-105.489121</td>\n",
              "      <td>-109.483409</td>\n",
              "      <td>-111.872877</td>\n",
              "      <td>-112.797498</td>\n",
              "      <td>-109.575000</td>\n",
              "      <td>-104.642421</td>\n",
              "      <td>-100.933292</td>\n",
              "      <td>-97.084572</td>\n",
              "      <td>-92.743601</td>\n",
              "      <td>-81.007031</td>\n",
              "      <td>-54.700801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-19.111807</td>\n",
              "      <td>-18.207809</td>\n",
              "      <td>-30.000353</td>\n",
              "      <td>-56.656145</td>\n",
              "      <td>-84.945021</td>\n",
              "      <td>-96.285853</td>\n",
              "      <td>-89.663072</td>\n",
              "      <td>-78.469302</td>\n",
              "      <td>-76.033125</td>\n",
              "      <td>-83.550953</td>\n",
              "      <td>-92.277465</td>\n",
              "      <td>-92.259688</td>\n",
              "      <td>-75.563280</td>\n",
              "      <td>-45.598032</td>\n",
              "      <td>-21.493130</td>\n",
              "      <td>-22.017917</td>\n",
              "      <td>-44.993030</td>\n",
              "      <td>-75.135987</td>\n",
              "      <td>-97.188919</td>\n",
              "      <td>-102.957234</td>\n",
              "      <td>-97.297153</td>\n",
              "      <td>-91.777842</td>\n",
              "      <td>-92.940752</td>\n",
              "      <td>-97.916864</td>\n",
              "      <td>-106.404980</td>\n",
              "      <td>-120.079267</td>\n",
              "      <td>-128.409065</td>\n",
              "      <td>-117.394749</td>\n",
              "      <td>-86.015558</td>\n",
              "      <td>-43.103243</td>\n",
              "      <td>-6.152475</td>\n",
              "      <td>6.430013</td>\n",
              "      <td>-2.804282</td>\n",
              "      <td>-16.792294</td>\n",
              "      <td>-26.781028</td>\n",
              "      <td>-34.573228</td>\n",
              "      <td>-43.125641</td>\n",
              "      <td>-55.173301</td>\n",
              "      <td>-69.087507</td>\n",
              "      <td>-77.621887</td>\n",
              "      <td>...</td>\n",
              "      <td>-62.678058</td>\n",
              "      <td>-60.486518</td>\n",
              "      <td>-61.853732</td>\n",
              "      <td>-64.081907</td>\n",
              "      <td>-65.390218</td>\n",
              "      <td>-64.989676</td>\n",
              "      <td>-56.427339</td>\n",
              "      <td>-29.914917</td>\n",
              "      <td>12.770838</td>\n",
              "      <td>55.127195</td>\n",
              "      <td>82.856669</td>\n",
              "      <td>90.091886</td>\n",
              "      <td>76.171049</td>\n",
              "      <td>46.906970</td>\n",
              "      <td>7.524796</td>\n",
              "      <td>-35.745489</td>\n",
              "      <td>-77.423244</td>\n",
              "      <td>-111.123138</td>\n",
              "      <td>-129.363487</td>\n",
              "      <td>-132.023401</td>\n",
              "      <td>-121.575471</td>\n",
              "      <td>-102.063389</td>\n",
              "      <td>-78.620203</td>\n",
              "      <td>-54.688683</td>\n",
              "      <td>-27.288849</td>\n",
              "      <td>5.461592</td>\n",
              "      <td>32.679762</td>\n",
              "      <td>44.529879</td>\n",
              "      <td>40.013309</td>\n",
              "      <td>24.020033</td>\n",
              "      <td>1.455554</td>\n",
              "      <td>-25.207038</td>\n",
              "      <td>-50.470021</td>\n",
              "      <td>-70.380416</td>\n",
              "      <td>-84.078753</td>\n",
              "      <td>-89.367751</td>\n",
              "      <td>-83.913087</td>\n",
              "      <td>-55.224658</td>\n",
              "      <td>6.302030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-15.947554</td>\n",
              "      <td>-32.853200</td>\n",
              "      <td>-50.193382</td>\n",
              "      <td>-67.306647</td>\n",
              "      <td>-77.305944</td>\n",
              "      <td>-73.469795</td>\n",
              "      <td>-57.727367</td>\n",
              "      <td>-39.256632</td>\n",
              "      <td>-24.812316</td>\n",
              "      <td>-17.056586</td>\n",
              "      <td>-12.789972</td>\n",
              "      <td>-7.667105</td>\n",
              "      <td>-5.712115</td>\n",
              "      <td>-9.022592</td>\n",
              "      <td>-12.093573</td>\n",
              "      <td>-12.761824</td>\n",
              "      <td>-13.876444</td>\n",
              "      <td>-17.429609</td>\n",
              "      <td>-20.736191</td>\n",
              "      <td>-17.963344</td>\n",
              "      <td>-6.988598</td>\n",
              "      <td>11.579035</td>\n",
              "      <td>29.944793</td>\n",
              "      <td>34.565892</td>\n",
              "      <td>18.702019</td>\n",
              "      <td>-9.311244</td>\n",
              "      <td>-33.502032</td>\n",
              "      <td>-47.507091</td>\n",
              "      <td>-48.365108</td>\n",
              "      <td>-37.742119</td>\n",
              "      <td>-29.960480</td>\n",
              "      <td>-36.400364</td>\n",
              "      <td>-54.595447</td>\n",
              "      <td>-75.659469</td>\n",
              "      <td>-88.035607</td>\n",
              "      <td>-79.777258</td>\n",
              "      <td>-51.921248</td>\n",
              "      <td>-18.714378</td>\n",
              "      <td>5.670264</td>\n",
              "      <td>14.947645</td>\n",
              "      <td>...</td>\n",
              "      <td>-71.644879</td>\n",
              "      <td>-58.481542</td>\n",
              "      <td>-46.606258</td>\n",
              "      <td>-44.297749</td>\n",
              "      <td>-54.740928</td>\n",
              "      <td>-71.172883</td>\n",
              "      <td>-88.032228</td>\n",
              "      <td>-101.596335</td>\n",
              "      <td>-101.706684</td>\n",
              "      <td>-85.142958</td>\n",
              "      <td>-60.045273</td>\n",
              "      <td>-36.041617</td>\n",
              "      <td>-23.406743</td>\n",
              "      <td>-30.728950</td>\n",
              "      <td>-54.241079</td>\n",
              "      <td>-78.579380</td>\n",
              "      <td>-92.078663</td>\n",
              "      <td>-90.871136</td>\n",
              "      <td>-71.999574</td>\n",
              "      <td>-41.123946</td>\n",
              "      <td>-13.123219</td>\n",
              "      <td>-0.337803</td>\n",
              "      <td>-7.353696</td>\n",
              "      <td>-27.741516</td>\n",
              "      <td>-53.012072</td>\n",
              "      <td>-74.939634</td>\n",
              "      <td>-87.965132</td>\n",
              "      <td>-96.203803</td>\n",
              "      <td>-101.024284</td>\n",
              "      <td>-98.974551</td>\n",
              "      <td>-89.661428</td>\n",
              "      <td>-77.224521</td>\n",
              "      <td>-67.862390</td>\n",
              "      <td>-64.388712</td>\n",
              "      <td>-69.512189</td>\n",
              "      <td>-87.782978</td>\n",
              "      <td>-115.443244</td>\n",
              "      <td>-112.612378</td>\n",
              "      <td>-47.192246</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.772188</td>\n",
              "      <td>20.190055</td>\n",
              "      <td>28.287400</td>\n",
              "      <td>31.096170</td>\n",
              "      <td>28.452101</td>\n",
              "      <td>18.935517</td>\n",
              "      <td>7.650470</td>\n",
              "      <td>-0.141660</td>\n",
              "      <td>-9.785346</td>\n",
              "      <td>-27.421504</td>\n",
              "      <td>-50.548138</td>\n",
              "      <td>-71.771816</td>\n",
              "      <td>-82.044855</td>\n",
              "      <td>-72.389977</td>\n",
              "      <td>-46.096924</td>\n",
              "      <td>-18.388917</td>\n",
              "      <td>-2.570595</td>\n",
              "      <td>-0.914791</td>\n",
              "      <td>-7.594949</td>\n",
              "      <td>-18.825438</td>\n",
              "      <td>-30.210385</td>\n",
              "      <td>-39.198186</td>\n",
              "      <td>-48.241527</td>\n",
              "      <td>-57.134728</td>\n",
              "      <td>-60.411707</td>\n",
              "      <td>-55.568373</td>\n",
              "      <td>-43.666973</td>\n",
              "      <td>-26.682292</td>\n",
              "      <td>-9.885465</td>\n",
              "      <td>-0.774389</td>\n",
              "      <td>-3.928756</td>\n",
              "      <td>-14.472251</td>\n",
              "      <td>-27.304343</td>\n",
              "      <td>-40.323567</td>\n",
              "      <td>-50.461036</td>\n",
              "      <td>-56.751957</td>\n",
              "      <td>-60.998713</td>\n",
              "      <td>-64.745609</td>\n",
              "      <td>-66.228619</td>\n",
              "      <td>-63.835649</td>\n",
              "      <td>...</td>\n",
              "      <td>43.258186</td>\n",
              "      <td>47.694162</td>\n",
              "      <td>51.324347</td>\n",
              "      <td>53.346104</td>\n",
              "      <td>51.787230</td>\n",
              "      <td>41.196653</td>\n",
              "      <td>24.281222</td>\n",
              "      <td>10.676972</td>\n",
              "      <td>3.443025</td>\n",
              "      <td>-0.946569</td>\n",
              "      <td>-2.410442</td>\n",
              "      <td>0.590403</td>\n",
              "      <td>4.778121</td>\n",
              "      <td>8.933470</td>\n",
              "      <td>17.778446</td>\n",
              "      <td>33.972499</td>\n",
              "      <td>49.844720</td>\n",
              "      <td>56.750368</td>\n",
              "      <td>55.022614</td>\n",
              "      <td>48.547416</td>\n",
              "      <td>42.278659</td>\n",
              "      <td>40.352384</td>\n",
              "      <td>40.372924</td>\n",
              "      <td>38.573360</td>\n",
              "      <td>40.156330</td>\n",
              "      <td>47.758084</td>\n",
              "      <td>52.922673</td>\n",
              "      <td>51.624529</td>\n",
              "      <td>48.181252</td>\n",
              "      <td>45.416014</td>\n",
              "      <td>41.665347</td>\n",
              "      <td>35.520503</td>\n",
              "      <td>27.382387</td>\n",
              "      <td>18.119056</td>\n",
              "      <td>10.371055</td>\n",
              "      <td>6.490683</td>\n",
              "      <td>6.065100</td>\n",
              "      <td>10.335592</td>\n",
              "      <td>21.756116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>39.651971</td>\n",
              "      <td>25.811583</td>\n",
              "      <td>17.381123</td>\n",
              "      <td>26.598736</td>\n",
              "      <td>59.605076</td>\n",
              "      <td>107.002509</td>\n",
              "      <td>150.715671</td>\n",
              "      <td>177.216425</td>\n",
              "      <td>186.965757</td>\n",
              "      <td>190.497574</td>\n",
              "      <td>199.622880</td>\n",
              "      <td>215.481126</td>\n",
              "      <td>226.788652</td>\n",
              "      <td>221.911213</td>\n",
              "      <td>196.586825</td>\n",
              "      <td>154.935283</td>\n",
              "      <td>106.806065</td>\n",
              "      <td>63.403343</td>\n",
              "      <td>33.908803</td>\n",
              "      <td>22.339514</td>\n",
              "      <td>26.815997</td>\n",
              "      <td>41.649318</td>\n",
              "      <td>59.622565</td>\n",
              "      <td>71.098310</td>\n",
              "      <td>67.955211</td>\n",
              "      <td>50.533758</td>\n",
              "      <td>22.345443</td>\n",
              "      <td>-10.646529</td>\n",
              "      <td>-35.606444</td>\n",
              "      <td>-41.395530</td>\n",
              "      <td>-22.863477</td>\n",
              "      <td>16.528275</td>\n",
              "      <td>59.679727</td>\n",
              "      <td>91.719370</td>\n",
              "      <td>111.802087</td>\n",
              "      <td>128.804737</td>\n",
              "      <td>150.930503</td>\n",
              "      <td>176.478483</td>\n",
              "      <td>194.829317</td>\n",
              "      <td>197.207316</td>\n",
              "      <td>...</td>\n",
              "      <td>9.976442</td>\n",
              "      <td>42.883962</td>\n",
              "      <td>56.415058</td>\n",
              "      <td>47.574538</td>\n",
              "      <td>27.115104</td>\n",
              "      <td>13.547972</td>\n",
              "      <td>12.504006</td>\n",
              "      <td>10.584986</td>\n",
              "      <td>-10.201100</td>\n",
              "      <td>-54.782426</td>\n",
              "      <td>-115.882838</td>\n",
              "      <td>-181.811262</td>\n",
              "      <td>-240.080378</td>\n",
              "      <td>-280.629102</td>\n",
              "      <td>-297.489250</td>\n",
              "      <td>-290.614828</td>\n",
              "      <td>-268.419100</td>\n",
              "      <td>-244.447197</td>\n",
              "      <td>-228.053779</td>\n",
              "      <td>-220.150789</td>\n",
              "      <td>-215.095065</td>\n",
              "      <td>-206.775868</td>\n",
              "      <td>-192.045535</td>\n",
              "      <td>-172.900746</td>\n",
              "      <td>-155.968747</td>\n",
              "      <td>-149.252191</td>\n",
              "      <td>-156.910158</td>\n",
              "      <td>-173.833299</td>\n",
              "      <td>-191.952187</td>\n",
              "      <td>-201.441621</td>\n",
              "      <td>-192.316456</td>\n",
              "      <td>-163.618764</td>\n",
              "      <td>-124.249019</td>\n",
              "      <td>-86.256351</td>\n",
              "      <td>-59.223875</td>\n",
              "      <td>-48.923595</td>\n",
              "      <td>-52.984326</td>\n",
              "      <td>-69.575668</td>\n",
              "      <td>-100.039746</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>-0.730217</td>\n",
              "      <td>-9.660417</td>\n",
              "      <td>-20.812752</td>\n",
              "      <td>-35.417980</td>\n",
              "      <td>-53.093451</td>\n",
              "      <td>-70.194301</td>\n",
              "      <td>-82.052064</td>\n",
              "      <td>-88.371895</td>\n",
              "      <td>-92.689832</td>\n",
              "      <td>-98.675122</td>\n",
              "      <td>-104.918682</td>\n",
              "      <td>-106.442153</td>\n",
              "      <td>-102.644730</td>\n",
              "      <td>-102.539343</td>\n",
              "      <td>-122.520214</td>\n",
              "      <td>-174.261931</td>\n",
              "      <td>-251.000199</td>\n",
              "      <td>-332.121861</td>\n",
              "      <td>-390.097470</td>\n",
              "      <td>-403.495170</td>\n",
              "      <td>-369.703386</td>\n",
              "      <td>-300.969786</td>\n",
              "      <td>-221.236986</td>\n",
              "      <td>-146.565494</td>\n",
              "      <td>-81.945339</td>\n",
              "      <td>-36.065759</td>\n",
              "      <td>-12.126853</td>\n",
              "      <td>-0.179020</td>\n",
              "      <td>9.921127</td>\n",
              "      <td>21.203827</td>\n",
              "      <td>35.481534</td>\n",
              "      <td>56.792899</td>\n",
              "      <td>84.387115</td>\n",
              "      <td>109.640787</td>\n",
              "      <td>126.225514</td>\n",
              "      <td>133.878629</td>\n",
              "      <td>135.058645</td>\n",
              "      <td>130.543507</td>\n",
              "      <td>119.572977</td>\n",
              "      <td>106.306276</td>\n",
              "      <td>...</td>\n",
              "      <td>-131.802265</td>\n",
              "      <td>-186.173700</td>\n",
              "      <td>-313.014259</td>\n",
              "      <td>-495.325700</td>\n",
              "      <td>-667.719375</td>\n",
              "      <td>-750.473673</td>\n",
              "      <td>-724.606174</td>\n",
              "      <td>-655.920869</td>\n",
              "      <td>-624.083314</td>\n",
              "      <td>-665.500760</td>\n",
              "      <td>-759.346913</td>\n",
              "      <td>-851.084944</td>\n",
              "      <td>-894.842194</td>\n",
              "      <td>-879.651136</td>\n",
              "      <td>-823.064277</td>\n",
              "      <td>-744.909993</td>\n",
              "      <td>-656.858419</td>\n",
              "      <td>-571.345079</td>\n",
              "      <td>-513.710445</td>\n",
              "      <td>-504.202767</td>\n",
              "      <td>-533.685817</td>\n",
              "      <td>-564.391744</td>\n",
              "      <td>-546.045305</td>\n",
              "      <td>-460.698055</td>\n",
              "      <td>-340.999374</td>\n",
              "      <td>-238.261260</td>\n",
              "      <td>-190.925946</td>\n",
              "      <td>-223.525169</td>\n",
              "      <td>-327.988232</td>\n",
              "      <td>-455.215666</td>\n",
              "      <td>-551.972574</td>\n",
              "      <td>-565.242173</td>\n",
              "      <td>-474.888547</td>\n",
              "      <td>-322.721541</td>\n",
              "      <td>-173.807843</td>\n",
              "      <td>-87.819622</td>\n",
              "      <td>-86.791321</td>\n",
              "      <td>-153.209241</td>\n",
              "      <td>-259.991392</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>75.046048</td>\n",
              "      <td>43.457087</td>\n",
              "      <td>7.410589</td>\n",
              "      <td>-32.846356</td>\n",
              "      <td>-81.832623</td>\n",
              "      <td>-141.325716</td>\n",
              "      <td>-187.748208</td>\n",
              "      <td>-183.605325</td>\n",
              "      <td>-109.669941</td>\n",
              "      <td>32.742767</td>\n",
              "      <td>214.615817</td>\n",
              "      <td>373.443334</td>\n",
              "      <td>451.390741</td>\n",
              "      <td>424.935981</td>\n",
              "      <td>313.735471</td>\n",
              "      <td>174.282954</td>\n",
              "      <td>53.354169</td>\n",
              "      <td>-33.140219</td>\n",
              "      <td>-82.756000</td>\n",
              "      <td>-101.144358</td>\n",
              "      <td>-105.433254</td>\n",
              "      <td>-111.643740</td>\n",
              "      <td>-127.467261</td>\n",
              "      <td>-152.616694</td>\n",
              "      <td>-178.362860</td>\n",
              "      <td>-196.394188</td>\n",
              "      <td>-209.695762</td>\n",
              "      <td>-237.847934</td>\n",
              "      <td>-296.836683</td>\n",
              "      <td>-384.926432</td>\n",
              "      <td>-479.873017</td>\n",
              "      <td>-557.306778</td>\n",
              "      <td>-608.057785</td>\n",
              "      <td>-631.751011</td>\n",
              "      <td>-638.280173</td>\n",
              "      <td>-640.521533</td>\n",
              "      <td>-649.282954</td>\n",
              "      <td>-667.954428</td>\n",
              "      <td>-683.797336</td>\n",
              "      <td>-665.055090</td>\n",
              "      <td>...</td>\n",
              "      <td>283.079827</td>\n",
              "      <td>311.727448</td>\n",
              "      <td>264.294232</td>\n",
              "      <td>185.209845</td>\n",
              "      <td>102.043981</td>\n",
              "      <td>25.715545</td>\n",
              "      <td>-43.559309</td>\n",
              "      <td>-103.698641</td>\n",
              "      <td>-149.972208</td>\n",
              "      <td>-176.261799</td>\n",
              "      <td>-174.996310</td>\n",
              "      <td>-147.708800</td>\n",
              "      <td>-111.417367</td>\n",
              "      <td>-91.233162</td>\n",
              "      <td>-99.294389</td>\n",
              "      <td>-133.362045</td>\n",
              "      <td>-190.255085</td>\n",
              "      <td>-267.433988</td>\n",
              "      <td>-356.568720</td>\n",
              "      <td>-441.237054</td>\n",
              "      <td>-505.112416</td>\n",
              "      <td>-534.997829</td>\n",
              "      <td>-531.125913</td>\n",
              "      <td>-506.747257</td>\n",
              "      <td>-478.241452</td>\n",
              "      <td>-475.215685</td>\n",
              "      <td>-527.698695</td>\n",
              "      <td>-629.836162</td>\n",
              "      <td>-723.992610</td>\n",
              "      <td>-723.569159</td>\n",
              "      <td>-591.814838</td>\n",
              "      <td>-386.664505</td>\n",
              "      <td>-197.223327</td>\n",
              "      <td>-74.387746</td>\n",
              "      <td>-10.345872</td>\n",
              "      <td>18.219158</td>\n",
              "      <td>-0.093097</td>\n",
              "      <td>-113.567889</td>\n",
              "      <td>-337.530871</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>619.260498</td>\n",
              "      <td>309.592902</td>\n",
              "      <td>94.504875</td>\n",
              "      <td>74.550116</td>\n",
              "      <td>334.333088</td>\n",
              "      <td>832.159320</td>\n",
              "      <td>1420.340795</td>\n",
              "      <td>1934.317296</td>\n",
              "      <td>2262.074766</td>\n",
              "      <td>2384.486829</td>\n",
              "      <td>2342.414676</td>\n",
              "      <td>2191.648937</td>\n",
              "      <td>1979.121111</td>\n",
              "      <td>1739.658864</td>\n",
              "      <td>1501.290265</td>\n",
              "      <td>1279.122522</td>\n",
              "      <td>1076.865679</td>\n",
              "      <td>899.012856</td>\n",
              "      <td>747.233776</td>\n",
              "      <td>622.850469</td>\n",
              "      <td>524.868115</td>\n",
              "      <td>447.657843</td>\n",
              "      <td>387.608333</td>\n",
              "      <td>349.371659</td>\n",
              "      <td>349.993455</td>\n",
              "      <td>392.815725</td>\n",
              "      <td>442.290166</td>\n",
              "      <td>426.333063</td>\n",
              "      <td>263.896323</td>\n",
              "      <td>-43.858911</td>\n",
              "      <td>-353.839791</td>\n",
              "      <td>-426.383120</td>\n",
              "      <td>-106.958380</td>\n",
              "      <td>505.852663</td>\n",
              "      <td>1154.358480</td>\n",
              "      <td>1616.177329</td>\n",
              "      <td>1835.707110</td>\n",
              "      <td>1881.631185</td>\n",
              "      <td>1818.532038</td>\n",
              "      <td>1687.269371</td>\n",
              "      <td>...</td>\n",
              "      <td>-882.429785</td>\n",
              "      <td>-862.084644</td>\n",
              "      <td>-847.434373</td>\n",
              "      <td>-850.186183</td>\n",
              "      <td>-872.509602</td>\n",
              "      <td>-908.601894</td>\n",
              "      <td>-948.657352</td>\n",
              "      <td>-980.629029</td>\n",
              "      <td>-1005.415316</td>\n",
              "      <td>-1044.736533</td>\n",
              "      <td>-1128.469961</td>\n",
              "      <td>-1278.554048</td>\n",
              "      <td>-1479.693961</td>\n",
              "      <td>-1682.243030</td>\n",
              "      <td>-1825.497187</td>\n",
              "      <td>-1853.665830</td>\n",
              "      <td>-1748.774784</td>\n",
              "      <td>-1535.687124</td>\n",
              "      <td>-1267.787168</td>\n",
              "      <td>-1008.944249</td>\n",
              "      <td>-815.895743</td>\n",
              "      <td>-727.186154</td>\n",
              "      <td>-744.692644</td>\n",
              "      <td>-841.486441</td>\n",
              "      <td>-982.605215</td>\n",
              "      <td>-1128.114218</td>\n",
              "      <td>-1247.207674</td>\n",
              "      <td>-1331.488815</td>\n",
              "      <td>-1385.291821</td>\n",
              "      <td>-1418.866625</td>\n",
              "      <td>-1445.403021</td>\n",
              "      <td>-1473.406066</td>\n",
              "      <td>-1504.369436</td>\n",
              "      <td>-1535.357527</td>\n",
              "      <td>-1563.661130</td>\n",
              "      <td>-1591.091849</td>\n",
              "      <td>-1605.064705</td>\n",
              "      <td>-1377.085434</td>\n",
              "      <td>-708.532988</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>-83.947354</td>\n",
              "      <td>-109.592462</td>\n",
              "      <td>-187.256530</td>\n",
              "      <td>-303.852030</td>\n",
              "      <td>-418.279062</td>\n",
              "      <td>-515.214539</td>\n",
              "      <td>-588.699021</td>\n",
              "      <td>-624.738059</td>\n",
              "      <td>-630.265487</td>\n",
              "      <td>-617.559888</td>\n",
              "      <td>-588.566864</td>\n",
              "      <td>-546.470220</td>\n",
              "      <td>-494.121036</td>\n",
              "      <td>-441.106554</td>\n",
              "      <td>-399.335478</td>\n",
              "      <td>-369.366632</td>\n",
              "      <td>-341.187171</td>\n",
              "      <td>-306.406167</td>\n",
              "      <td>-265.193200</td>\n",
              "      <td>-222.175677</td>\n",
              "      <td>-180.633218</td>\n",
              "      <td>-142.180945</td>\n",
              "      <td>-111.163092</td>\n",
              "      <td>-91.671668</td>\n",
              "      <td>-81.389881</td>\n",
              "      <td>-74.628303</td>\n",
              "      <td>-67.845948</td>\n",
              "      <td>-60.105654</td>\n",
              "      <td>-50.971450</td>\n",
              "      <td>-41.473557</td>\n",
              "      <td>-34.256424</td>\n",
              "      <td>-31.567651</td>\n",
              "      <td>-34.656542</td>\n",
              "      <td>-40.259729</td>\n",
              "      <td>-41.291546</td>\n",
              "      <td>-33.991815</td>\n",
              "      <td>-18.798674</td>\n",
              "      <td>0.524934</td>\n",
              "      <td>14.486447</td>\n",
              "      <td>13.599810</td>\n",
              "      <td>...</td>\n",
              "      <td>455.882356</td>\n",
              "      <td>475.986338</td>\n",
              "      <td>481.083050</td>\n",
              "      <td>478.989700</td>\n",
              "      <td>487.697730</td>\n",
              "      <td>520.907835</td>\n",
              "      <td>580.513944</td>\n",
              "      <td>660.968562</td>\n",
              "      <td>753.090680</td>\n",
              "      <td>843.874997</td>\n",
              "      <td>916.329424</td>\n",
              "      <td>953.736537</td>\n",
              "      <td>947.416582</td>\n",
              "      <td>902.968044</td>\n",
              "      <td>840.049432</td>\n",
              "      <td>779.776182</td>\n",
              "      <td>734.440890</td>\n",
              "      <td>711.387454</td>\n",
              "      <td>715.616874</td>\n",
              "      <td>751.742232</td>\n",
              "      <td>814.627296</td>\n",
              "      <td>876.142536</td>\n",
              "      <td>893.961181</td>\n",
              "      <td>824.834259</td>\n",
              "      <td>613.829808</td>\n",
              "      <td>213.386365</td>\n",
              "      <td>-330.631595</td>\n",
              "      <td>-853.032915</td>\n",
              "      <td>-1159.687298</td>\n",
              "      <td>-1139.259125</td>\n",
              "      <td>-819.257524</td>\n",
              "      <td>-339.778969</td>\n",
              "      <td>125.961112</td>\n",
              "      <td>460.949478</td>\n",
              "      <td>643.089731</td>\n",
              "      <td>716.630627</td>\n",
              "      <td>712.688155</td>\n",
              "      <td>377.923813</td>\n",
              "      <td>-507.309964</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 4098 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0           1           2  ...         4095        4096  Label\n",
              "0    -15.979399  -25.154399  -39.171428  ...    75.137893   51.276260      0\n",
              "1     -9.449462  -15.923934  -23.698261  ...   -81.007031  -54.700801      0\n",
              "2    -19.111807  -18.207809  -30.000353  ...   -55.224658    6.302030      0\n",
              "3    -15.947554  -32.853200  -50.193382  ...  -112.612378  -47.192246      0\n",
              "4      7.772188   20.190055   28.287400  ...    10.335592   21.756116      0\n",
              "..          ...         ...         ...  ...          ...         ...    ...\n",
              "495   39.651971   25.811583   17.381123  ...   -69.575668 -100.039746      1\n",
              "496   -0.730217   -9.660417  -20.812752  ...  -153.209241 -259.991392      1\n",
              "497   75.046048   43.457087    7.410589  ...  -113.567889 -337.530871      1\n",
              "498  619.260498  309.592902   94.504875  ... -1377.085434 -708.532988      1\n",
              "499  -83.947354 -109.592462 -187.256530  ...   377.923813 -507.309964      1\n",
              "\n",
              "[500 rows x 4098 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB1wRAPmq3qh",
        "outputId": "38a04d67-8b6f-4a45-8f31-ed00e985e874"
      },
      "source": [
        "df.columns\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
              "       ...\n",
              "       '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096',\n",
              "       'Label'],\n",
              "      dtype='object', length=4098)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eEvvTFrBWI"
      },
      "source": [
        "df = df.fillna(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5g0rSrhIrDog"
      },
      "source": [
        "df = df.sample(frac=1.0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfGfTwIarF3x",
        "outputId": "0a411471-9bcb-4792-af2d-95ec58db581b"
      },
      "source": [
        "X = df[df.columns[:-1]].values\n",
        "X.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 4097)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmit9tdSrWFI",
        "outputId": "73bd486e-714e-4eaa-d63b-9bdd3a9d879d"
      },
      "source": [
        "X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\n",
        "df.columns[:6000]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
              "       ...\n",
              "       '4088', '4089', '4090', '4091', '4092', '4093', '4094', '4095', '4096',\n",
              "       'Label'],\n",
              "      dtype='object', length=4098)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3IpGk12rZi2",
        "outputId": "a73c0d96-42ce-4619-e0a7-afa0eafa3903"
      },
      "source": [
        "y = df['Label'].values\n",
        "\n",
        "y[0:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwzobHIvrlr8"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c0Gog8Rrdt8"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3eqNhqNro33",
        "outputId": "47d7b3ba-fcfd-440d-d7ba-44149d7c8933"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=4)\n",
        "print ('Train set:', X_train.shape,  y_train.shape)\n",
        "print ('Test set:', X_test.shape,  y_test.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set: (335, 4097) (335,)\n",
            "Test set: (165, 4097) (165,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn7kf1sYrrRc",
        "outputId": "9371dffa-1759-458b-f87f-b19dc593696a"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "Ks = 10\n",
        "mean_acc = np.zeros((Ks-1))\n",
        "std_acc = np.zeros((Ks-1))\n",
        "ConfustionMx = [];\n",
        "for n in range(1,Ks):\n",
        "    \n",
        "    #Train Model and Predict  \n",
        "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
        "    \n",
        "    yhat=neigh.predict(X_test)\n",
        "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
        "\n",
        "    \n",
        "    std_acc[n-1]=np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
        "\n",
        "print(mean_acc)\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.92 0.85 0.85 0.82 0.82 0.79 0.79 0.79 0.79]\n",
            "0.44134078212290506\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      1.00      0.88        79\n",
            "           1       0.00      0.00      0.00        21\n",
            "\n",
            "    accuracy                           0.79       100\n",
            "   macro avg       0.40      0.50      0.44       100\n",
            "weighted avg       0.62      0.79      0.70       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr8E6FEqr1Aq"
      },
      "source": [
        "Gradient Boosting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzLnHMBmrvAm"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tqx2Rear5MX",
        "outputId": "a10a081d-c464-41bd-b674-1253675f8142"
      },
      "source": [
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0).fit(X_train, y_train)\n",
        "print(clf.score(X_train,y_train))#trianing dataset\n",
        "print(clf.score(X_test, y_test))# test accuracy \n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "1.0\n",
            "0.4350282485875706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      1.00      0.87        77\n",
            "           1       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.77       100\n",
            "   macro avg       0.39      0.50      0.44       100\n",
            "weighted avg       0.59      0.77      0.67       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAvOutiesCIJ"
      },
      "source": [
        "Adaboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfDSNMZ6r7D5"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk07i6qGsFBW",
        "outputId": "40e3cad2-e781-408b-f1e5-543441e3d00a"
      },
      "source": [
        "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "clf.fit(X_train,y_train)\n",
        "a=clf.predict(X_test)\n",
        "print(clf.score(X_test, y_test))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      1.00      0.87        77\n",
            "           1       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.77       100\n",
            "   macro avg       0.39      0.50      0.44       100\n",
            "weighted avg       0.59      0.77      0.67       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aU_gVaCsLUJ"
      },
      "source": [
        "Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z-qkO1lsHDd"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouUiQDXosOJ_",
        "outputId": "46258006-2a87-4c70-f6dd-c1ab89383469"
      },
      "source": [
        "grade = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n",
        "grade.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuNQg4QysQYf"
      },
      "source": [
        "predTree = grade.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuMZyRgesTjL",
        "outputId": "8b5a549b-a9b6-401c-e784-f8218f0524e4"
      },
      "source": [
        "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTrees's Accuracy:  1.0\n",
            "0.4350282485875706\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      1.00      0.87        77\n",
            "           1       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.77       100\n",
            "   macro avg       0.39      0.50      0.44       100\n",
            "weighted avg       0.59      0.77      0.67       100\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU7jOt6qsXgR"
      },
      "source": [
        "SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ0LY-FMsVXH"
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FlZDXorsaYC",
        "outputId": "2c15a7ed-3957-4b77-cdb1-c782ffbe9d46"
      },
      "source": [
        "clf = svm.SVC(kernel='rbf')\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1slZKsNpsbyz"
      },
      "source": [
        "yhat = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qS2hzsjMsd9r",
        "outputId": "6cad502d-51f8-4593-c3b9-196575474d14"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "f1_score(y_test, yhat, average='weighted') \n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(clf.score(X_test,y_test))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.939722724532851\n",
            "0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        77\n",
            "           1       1.00      0.83      0.90        23\n",
            "\n",
            "    accuracy                           0.96       100\n",
            "   macro avg       0.98      0.91      0.94       100\n",
            "weighted avg       0.96      0.96      0.96       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PA0S23kwsha5"
      },
      "source": [
        "Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCbU9l6xsfhw"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lco3hBLjslFA",
        "outputId": "5ba18fb2-786d-45fd-816b-17c5e40de1ce"
      },
      "source": [
        "cl=GaussianNB()\n",
        "cl.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lmj9x2MsmiI"
      },
      "source": [
        "a=cl.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seQVIi3_sn5s",
        "outputId": "f7de9806-cbdd-4a94-8811-c4bd1c5ac699"
      },
      "source": [
        "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, a))\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTrees's Accuracy:  0.96\n",
            "0.939722724532851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        77\n",
            "           1       1.00      0.83      0.90        23\n",
            "\n",
            "    accuracy                           0.96       100\n",
            "   macro avg       0.98      0.91      0.94       100\n",
            "weighted avg       0.96      0.96      0.96       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvkzU8fJsuZh"
      },
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14IUpOmSspx8"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5XJ6y7VswgW",
        "outputId": "297071c7-484e-4383-c8ec-98b1e2e306c2"
      },
      "source": [
        "clf=LogisticRegression(random_state=11).fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPxo7SZ1sx6P"
      },
      "source": [
        "a=clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSJqv-KIsz9a",
        "outputId": "f96d3cbb-d41d-4bc5-bce4-b19e21c05306"
      },
      "source": [
        "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, a))\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTrees's Accuracy:  0.83\n",
            "0.939722724532851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        77\n",
            "           1       1.00      0.83      0.90        23\n",
            "\n",
            "    accuracy                           0.96       100\n",
            "   macro avg       0.98      0.91      0.94       100\n",
            "weighted avg       0.96      0.96      0.96       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZNPV5hDs6kE"
      },
      "source": [
        "SGD Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNpE7fjDs3Ox"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOw2YhFDs9gG"
      },
      "source": [
        "sgd=SGDClassifier(loss='modified_huber',shuffle=False, random_state=11)\n",
        "sgd.fit(X_train,y_train)\n",
        "y_pred=sgd.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oW4oKJts-tJ",
        "outputId": "9a715093-b44f-406a-9e73-ac3b85c62f2f"
      },
      "source": [
        "print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test,y_pred))\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DecisionTrees's Accuracy:  0.83\n",
            "0.939722724532851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        77\n",
            "           1       1.00      0.83      0.90        23\n",
            "\n",
            "    accuracy                           0.96       100\n",
            "   macro avg       0.98      0.91      0.94       100\n",
            "weighted avg       0.96      0.96      0.96       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm96tDjdtBFf"
      },
      "source": [
        "XGBOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELl8D_1ytAP6",
        "outputId": "8340f008-3316-4522-b31a-ab47e7f650b0"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(n_estimators=100)\n",
        "xgb.fit(X_train, y_train)\n",
        "preds = xgb.predict(X_test)\n",
        "acc_xgb = (preds == y_test).sum().astype(float) / len(preds)*100\n",
        "print(acc_xgb)\n",
        "print(f1_score(yhat,y_test,average=\"macro\"))\n",
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100.0\n",
            "0.939722724532851\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97        77\n",
            "           1       1.00      0.83      0.90        23\n",
            "\n",
            "    accuracy                           0.96       100\n",
            "   macro avg       0.98      0.91      0.94       100\n",
            "weighted avg       0.96      0.96      0.96       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMow3wS0tHmZ"
      },
      "source": [
        "ANN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY3T1PkRtF0J"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMacYgKlWoW"
      },
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(units = 6000, activation = 'sigmoid', input_shape = (X.shape[1],)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 12000, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 6000, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 3000, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 1500, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 750, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 186, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 93, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 46, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 23, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 10, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 5, activation = 'sigmoid'))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(units = 2, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Le99WI6nmbX2",
        "outputId": "8ad17be1-df0d-4316-a6de-557b5ec8c8a4"
      },
      "source": [
        "model.fit(X_train, y_train, epochs = 100, batch_size = 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 11s 11s/step - loss: 0.5675 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5638 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5602 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5567 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5535 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5503 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5474 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5445 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5418 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5392 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5367 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5344 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5321 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5299 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5278 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5258 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5239 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5221 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.5204 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5187 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5172 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5157 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5142 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5129 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5116 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5103 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5091 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5080 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5070 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5059 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5050 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5041 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5032 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5024 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5016 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5009 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.5002 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4995 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4989 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4983 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4977 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4972 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4967 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4963 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4958 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4954 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4950 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4947 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4943 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4940 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4937 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4934 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4931 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4929 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4927 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4924 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4922 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4921 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4919 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4917 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4916 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4914 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4913 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4912 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4911 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4910 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4909 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4908 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4907 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4906 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4905 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4905 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4904 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4904 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4903 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 9s 9s/step - loss: 0.4903 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.4902 - sparse_categorical_accuracy: 0.8075\n",
            "Epoch 78/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-8bc1e087e0f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RVvB8tbmguV"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buHG_gofwLG4"
      },
      "source": [
        "print(classification_report(y_test, yhat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTFsoSXuwOoZ"
      },
      "source": [
        "X_train = X_train[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwNhIh-Uk9_1",
        "outputId": "706b95cf-906c-4c55-b976-2896c7ac48de"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(335, 4097, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLU80YLnmXN4"
      },
      "source": [
        "epochs = 20"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUSFT5oKcTRf"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Rfd_AixW5y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "outputId": "f3d2a27d-0c4b-4c66-f586-2218b67121c6"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(60,dropout = 0.2, input_shape = (X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy','mae'])\n",
        "print(model.summary)\n",
        "model.fit(X_train,y_train,epochs=epochs)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Model.summary of <keras.engine.sequential.Sequential object at 0x7f163d174d90>>\n",
            "Epoch 1/20\n",
            "11/11 [==============================] - 31s 2s/step - loss: 0.6648 - accuracy: 0.7970 - mae: 0.4843\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 30s 3s/step - loss: 0.5367 - accuracy: 0.8746 - mae: 0.4055\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 26s 2s/step - loss: 0.3175 - accuracy: 0.9075 - mae: 0.1403\n",
            "Epoch 4/20\n",
            " 8/11 [====================>.........] - ETA: 7s - loss: 0.2123 - accuracy: 0.9492 - mae: 0.1227"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-23f1623d75a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vF1NRAucDFQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}